{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "U-Net_seg",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-0kFp-fLOhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the data from the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f73TOPRWLdJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If this code runs and says \"Found GPU ...\" etc then congrats, you've turned the computation machine to full volume\n",
        "\n",
        "import tensorflow as tf # Importing our first module (as below) but we need it \n",
        "                        # earlier to check whether we have the GPU running in the correct place!\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PPYmOSrO25j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import the necessary libraries\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm #just to show the progress bar\n",
        "import sys\n",
        "import random\n",
        "from skimage.io import imread,imshow\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import asarray\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import backend as K\n",
        "from skimage.transform import rotate\n",
        "from skimage import color\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras import backend as keras\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DrWvOlYO60S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Different metrics that can be used for segmentation\n",
        "\n",
        "def dice(y_true, y_pred):\n",
        "        eps = 0.00001 \n",
        "        y_true_f = tf.reshape(y_true,[tf.shape(y_true)[0],tf.shape(y_true)[1]*tf.shape(y_true)[2]])\n",
        "        y_pred_f = tf.reshape(y_pred,[tf.shape(y_pred)[0],tf.shape(y_pred)[1]*tf.shape(y_pred)[2]])                                                                                                \n",
        "        intersection = eps + 2*tf.reduce_sum(y_true_f*y_pred_f, axis=1)                                                    \n",
        "        # eps added in denomintor, to take care for DivisionByZero error.\n",
        "        union = eps + tf.reduce_sum(y_true_f*y_true_f, axis=1) + tf.reduce_sum(y_pred_f*y_pred_f, axis=1)                                    \n",
        "        IOU = tf.math.divide(intersection,union)                                                                               \n",
        "        return (tf.reduce_mean(IOU))\n",
        "         \n",
        "def dice_loss(y_true, y_pred):\n",
        "        return -dice(y_true, y_pred)\n",
        "\n",
        "def dice_metric(y_true, y_pred):\n",
        "        y_true_f = K.cast(K.greater(y_true, 0.5), 'float32')\n",
        "        y_pred_f = K.cast(K.greater(y_pred, 0.5), 'float32')\n",
        "        \n",
        "        return dice(y_true_f, y_pred_f)\n",
        "\n",
        "def jacard_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
        "\n",
        "def jacard_coef_loss(y_true, y_pred):\n",
        "    return -jacard_coef(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl5zocOxPIdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#install the albumentations library\n",
        "pip install -U git+https://github.com/albu/albumentations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1jpMe8ESJTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import the augmentation tools\n",
        "from albumentations import (\n",
        "    HorizontalFlip,\n",
        "    VerticalFlip,    \n",
        "    CenterCrop,    \n",
        "    Compose,\n",
        "    Transpose,\n",
        "    RandomRotate90,\n",
        "    ElasticTransform,\n",
        "    GridDistortion, \n",
        "    OpticalDistortion,\n",
        "    RandomSizedCrop,\n",
        "    OneOf,\n",
        "    CLAHE,\n",
        "    RandomBrightnessContrast,    \n",
        "    RandomGamma    \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJMA2NvSTcCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#procedure for visualising different augmentations applied to an image\n",
        "def visualize(image, mask, original_image=None, original_mask=None):\n",
        "    fontsize = 18\n",
        "    \n",
        "    if original_image is None and original_mask is None:\n",
        "        f, ax = plt.subplots(2, 1, figsize=(8, 8))\n",
        "\n",
        "        ax[0].imshow(image)\n",
        "        ax[1].imshow(mask)\n",
        "    else:\n",
        "        f, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
        "\n",
        "        ax[0, 0].imshow(original_image, cmap='gray', vmin=0, vmax=255)\n",
        "        ax[0, 0].set_title('Original image', fontsize=fontsize)\n",
        "        \n",
        "        ax[1, 0].imshow(original_mask)\n",
        "        ax[1, 0].set_title('Original mask', fontsize=fontsize)\n",
        "        \n",
        "        ax[0, 1].imshow(image, cmap='gray', vmin=0, vmax=255)\n",
        "        ax[0, 1].set_title('Transformed image', fontsize=fontsize)\n",
        "        \n",
        "        ax[1, 1].imshow(mask)\n",
        "        ax[1, 1].set_title('Transformed mask', fontsize=fontsize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb2bK5qFmlto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image=imread(\"/content/drive/My Drive/3YP_Machine_Learning/PhC-C2DH-U373/01/t001.tif\")\n",
        "mask=imread(\"/content/drive/My Drive/3YP_Machine_Learning/PhC-C2DH-U373/01_ST/SEG/man_seg001.tif\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tu092F12M74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The procedure that applies augmentations to an image and mask where X is a training dataset and Y is the ground truth dataset\n",
        "def augmentation(image, mask, X, Y):\n",
        "\n",
        " #first degree of augmentations including the rotations...\n",
        "  image_medium1 = image[::-1, :] \n",
        "  mask_medium1 = mask[::-1, :] \n",
        "  #append them to the data set\n",
        "  X.append(image_medium1)\n",
        "  Y.append(mask_medium1)\n",
        "  image_medium10 = image[:, ::-1]\n",
        "  mask_medium10 = mask[:, ::-1]\n",
        "  #append them to the data set\n",
        "  image_medium10=img_to_array(image_medium10)\n",
        "  mask_medium10=img_to_array(mask_medium10)\n",
        "  X.append(image_medium10)\n",
        "  Y.append(mask_medium10)    \n",
        "  image_medium2 = rotate(image, 10)\n",
        "  mask_medium2 = rotate(mask, 10)\n",
        "  #append them to the data set\n",
        "  image_medium2=img_to_array(image_medium2)\n",
        "  mask_medium2=img_to_array(mask_medium2)\n",
        "  X.append(image_medium2)\n",
        "  Y.append(mask_medium2)\n",
        "  image_medium7 = rotate(image, -10)\n",
        "  mask_medium7 = rotate(mask, -10)\n",
        "  #append them to the data set\n",
        "  image_medium7=img_to_array(image_medium10)\n",
        "  mask_medium7=img_to_array(mask_medium10)\n",
        "  X.append(image_medium7)\n",
        "  Y.append(mask_medium7)\n",
        "  image_medium8 = rotate(image, 20)\n",
        "  mask_medium8 = rotate(mask, 20)\n",
        "  #append them to the data set\n",
        "  image_medium8=img_to_array(image_medium8)\n",
        "  mask_medium8=img_to_array(mask_medium8)\n",
        "  X.append(image_medium8)\n",
        "  Y.append(mask_medium8)\n",
        "  image_medium9 = rotate(image, -20)\n",
        "  mask_medium9 = rotate(mask, -20)\n",
        "  #append them to the data set\n",
        "  image_medium9=img_to_array(image_medium9)\n",
        "  mask_medium9=img_to_array(mask_medium9)\n",
        "  X.append(image_medium9)\n",
        "  Y.append(mask_medium9)\n",
        "  #second level of the augmentations including elastic deformations\n",
        "  aug3 = ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03)\n",
        "  augmented3 = aug3(image=image, mask=mask)\n",
        "  image_medium3 = augmented3['image']\n",
        "  mask_medium3 = augmented3['mask']\n",
        "  #append them to the dataset\n",
        "  image_medium3=img_to_array(image_medium3)\n",
        "  mask_medium3=img_to_array(mask_medium3)\n",
        "  X.append(image_medium3)\n",
        "  Y.append(mask_medium3)\n",
        "  aug4 = GridDistortion(p=1.5)\n",
        "  augmented4 = aug4(image=image, mask=mask)\n",
        "  image_medium4 = augmented4['image']\n",
        "  mask_medium4 = augmented4['mask']\n",
        "  #append them to the dataset\n",
        "  image_medium4=img_to_array(image_medium4)\n",
        "  mask_medium4=img_to_array(mask_medium4)\n",
        "  X.append(image_medium4)\n",
        "  Y.append(mask_medium4)\n",
        "  #aug5 = OpticalDistortion(p=1, distort_limit=1, shift_limit=0.5)\n",
        "  #augmented5 = aug5(image=image, mask=mask)\n",
        "  #image_medium5 = augmented5['image']\n",
        "  #mask_medium5 = augmented5['mask']\n",
        "  #append them to the dataset\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kznJ4VB16P65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This part of the code performs data loading and prepares the data for the neural netork\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "IMG_CHANNELS = 1\n",
        "\n",
        "TRAIN_PATH = \"/content/drive/My Drive/3YP_Machine_Learning/PhC-C2DH-U373/01\"\n",
        "MASKS_PATH = \"/content/drive/My Drive/3YP_Machine_Learning/PhC-C2DH-U373/01_ST/SEG\"\n",
        "TEST_DATA= \"/content/drive/My Drive/3YP_Machine_Learning/PhC-C2DH-U373_2/01\"\n",
        "\n",
        "trin_ids= os.listdir(TRAIN_PATH)\n",
        "test_ids= os.listdir(TEST_DATA)\n",
        "mask_ids= os.listdir(MASKS_PATH)\n",
        "\n",
        "X_train=list()\n",
        "Y_train=list()\n",
        "\n",
        "#loading in the training data\n",
        "\n",
        "for n in range(91):\n",
        "  image=imread(TRAIN_PATH +'/'+ train_ids[n])\n",
        "  image=resize(image, (IMG_HEIGHT,IMG_WIDTH,1))\n",
        "  image=img_to_array(image)\n",
        "  X_train.append(image)\n",
        "  if n<10:\n",
        "    s = '/man_seg00'+ str(n) +'.tif'\n",
        "  else:\n",
        "    s='/man_seg0' + str(n) + '.tif'\n",
        "  mask=65535*imread(MASKS_PATH +'/'+ mask_ids[n])\n",
        "  mask= resize(image, (IMG_HEIGHT,IMG_WIDTH,1))\n",
        "  mask=img_to_array(mask)\n",
        "  Y_train.append(mask)\n",
        "  augmentation(image, mask, X_train, Y_train)\n",
        "\n",
        "X_Train = np.array(X_train)\n",
        "Y_Train = np.array(Y_train)\n",
        " #loading in the test data\n",
        "X_test=list()\n",
        "for i in range(10):\n",
        "  if i<10:\n",
        "    s = '/t00'+ str(i) +'.tif'\n",
        "  else:\n",
        "    s='/t0' + str(i) + '.tif'\n",
        "  image_test=imread(TEST_DATA + s)\n",
        "  image_test=resize(image, (IMG_HEIGHT,IMG_WIDTH,1))\n",
        "  image_test=img_to_array(image_test)\n",
        "  X_test.append(image_test)\n",
        "\n",
        "X_Test=np.array(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYD5Lc-b7zDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sanity check is the data in the right format, if it is (no_of_images, size_x, size_y, number of channels).....then yes!\n",
        "print(X_Train.shape, Y_Train.shape, X_Test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrltk99T8h5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Build the model\n",
        "inputs = tf.keras.layers.Input((128, 128,1))\n",
        "s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "\n",
        "#Contraction path\n",
        "c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
        "c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
        "c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
        "c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        " \n",
        "c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
        "c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
        "c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        " \n",
        "c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
        "c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
        "c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        " \n",
        "c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
        "c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
        "c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "\n",
        "#Expansive path \n",
        "u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "u6 = tf.keras.layers.concatenate([u6, c4])\n",
        "c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
        "c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
        "c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
        " \n",
        "u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "u7 = tf.keras.layers.concatenate([u7, c3])\n",
        "c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
        "c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        " \n",
        "u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "u8 = tf.keras.layers.concatenate([u8, c2])\n",
        "c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
        "c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
        "c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
        " \n",
        "u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
        "c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
        "c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
        "c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
        " \n",
        "outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        " \n",
        "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics=[jacard_coef])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nGKJTYo8xUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpointer = tf.keras.callbacks.ModelCheckpoint('hosico1.h5', verbose=1)\n",
        "#callbacks1=[\n",
        "           #tf.keras.callbacks.EarlyStopping(patience=2,monitor='val_loss'),\n",
        "           #tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
        "#]\n",
        "results = model.fit(X_Train, Y_Train,batch_size=32, epochs=100,callbacks=checkpointer, shuffle=True) #choose suffle=true to shuffle the training images order"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}